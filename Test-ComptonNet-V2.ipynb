{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d8d0b-6be8-435b-aca4-59ef61d2ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchsummary import summary\n",
    "\n",
    "Project_Data = \"C:\\\\Users\\\\yazee\\\\Desktop\\\\PhD-Studies\\\\Data\"\n",
    "Project_Figure = \"C:\\\\Users\\\\yazee\\\\Desktop\\\\PhD-Studies\\\\Figure\"\n",
    "\n",
    "Project_Paths = [Project_Data, Project_Figure]\n",
    "for path in Project_Paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exists. Creating it ....\")\n",
    "        os.mkdir(path)\n",
    "\n",
    "file_names = [f for f in os.listdir(Project_Data) if f.endswith(\".txt\")]\n",
    "data_list = {}\n",
    "for fName in file_names:\n",
    "    key = fName.split('.')[0]\n",
    "    data_list[key] = pd.read_csv(os.path.join(Project_Data,fName), delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a22168-0ca5-4091-a72b-766b20f353e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComptonNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComptonNetV2, self).__init__()\n",
    "        \n",
    "        # MLP for both-reacted events (6 features)\n",
    "        self.mlp_both = nn.Sequential(\n",
    "            nn.Linear(6, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # MLP for scatter-only events (3 features)\n",
    "        self.mlp_scatter = nn.Sequential(\n",
    "            nn.Linear(3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # MLP for absorber-only events (3 features)\n",
    "        self.mlp_absorber = nn.Sequential(\n",
    "            nn.Linear(3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers for feature extraction and upsampling\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upsample1 = nn.Upsample(size=(16, 128), mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(size=(32, 256), mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(size=(64, 256), mode='bilinear', align_corners=True)\n",
    "        self.upsample4 = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x_both, x_scatter, x_absorber):\n",
    "        # Process both-reacted events\n",
    "        features_both = self.mlp_both(x_both)\n",
    "        features_both = torch.max(features_both, dim=0, keepdim=True)[0]\n",
    "        \n",
    "        # Process scatter-only events\n",
    "        features_scatter = self.mlp_scatter(x_scatter)\n",
    "        features_scatter = torch.max(features_scatter, dim=0, keepdim=True)[0]\n",
    "        \n",
    "        # Process absorber-only events\n",
    "        features_absorber = self.mlp_absorber(x_absorber)\n",
    "        features_absorber = torch.max(features_absorber, dim=0, keepdim=True)[0]\n",
    "        \n",
    "        # Concatenate features from all event types\n",
    "        combined_features = torch.cat([features_both, features_scatter, features_absorber], dim=1)\n",
    "        combined_features = combined_features.unsqueeze(1).unsqueeze(1)\n",
    "        \n",
    "        # Pass through the convolutional layers with upsampling\n",
    "        x = F.relu(self.bn1(self.conv1(combined_features)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample4(x)\n",
    "        \n",
    "        x = self.bn5(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def custom_summary(model):\n",
    "    print(f\"{'Layer':<25} {'Output Shape':<20} {'Param #':<10}\")\n",
    "    print(\"=\"*55)\n",
    "    \n",
    "    x_both = torch.zeros(1, 6)\n",
    "    x_scatter = torch.zeros(1, 3)\n",
    "    x_absorber = torch.zeros(1, 3)\n",
    "    \n",
    "    features_both = model.mlp_both(x_both)\n",
    "    features_both = torch.max(features_both, dim=0, keepdim=True)[0]\n",
    "    print(f\"{'mlp_both':<25} {str(features_both.shape):<20} {sum(p.numel() for p in model.mlp_both.parameters()):<10}\")\n",
    "    \n",
    "    features_scatter = model.mlp_scatter(x_scatter)\n",
    "    features_scatter = torch.max(features_scatter, dim=0, keepdim=True)[0]\n",
    "    print(f\"{'mlp_scatter':<25} {str(features_scatter.shape):<20} {sum(p.numel() for p in model.mlp_scatter.parameters()):<10}\")\n",
    "    \n",
    "    features_absorber = model.mlp_absorber(x_absorber)\n",
    "    features_absorber = torch.max(features_absorber, dim=0, keepdim=True)[0]\n",
    "    print(f\"{'mlp_absorber':<25} {str(features_absorber.shape):<20} {sum(p.numel() for p in model.mlp_absorber.parameters()):<10}\")\n",
    "    \n",
    "    combined_features = torch.cat([features_both, features_scatter, features_absorber], dim=1)\n",
    "    combined_features = combined_features.unsqueeze(1).unsqueeze(1)\n",
    "    print(f\"{'Concatenated Features':<25} {str(combined_features.shape):<20} {'-':<10}\")\n",
    "    \n",
    "    x = F.relu(model.bn1(model.conv1(combined_features)))\n",
    "    print(f\"{'conv1':<25} {str(x.shape):<20} {sum(p.numel() for p in model.conv1.parameters()):<10}\")\n",
    "    \n",
    "    x = model.dropout(x)\n",
    "    x = model.upsample1(x)\n",
    "    print(f\"{'upsample1':<25} {str(x.shape):<20} {'-':<10}\")\n",
    "    \n",
    "    x = F.relu(model.bn2(model.conv2(x)))\n",
    "    print(f\"{'conv2':<25} {str(x.shape):<20} {sum(p.numel() for p in model.conv2.parameters()):<10}\")\n",
    "    \n",
    "    x = model.dropout(x)\n",
    "    x = model.upsample2(x)\n",
    "    print(f\"{'upsample2':<25} {str(x.shape):<20} {'-':<10}\")\n",
    "    \n",
    "    x = F.relu(model.bn3(model.conv3(x)))\n",
    "    print(f\"{'conv3':<25} {str(x.shape):<20} {sum(p.numel() for p in model.conv3.parameters()):<10}\")\n",
    "    \n",
    "    x = model.dropout(x)\n",
    "    x = model.upsample3(x)\n",
    "    print(f\"{'upsample3':<25} {str(x.shape):<20} {'-':<10}\")\n",
    "    \n",
    "    x = F.relu(model.bn4(model.conv4(x)))\n",
    "    print(f\"{'conv4':<25} {str(x.shape):<20} {sum(p.numel() for p in model.conv4.parameters()):<10}\")\n",
    "    \n",
    "    x = model.dropout(x)\n",
    "    x = model.upsample4(x)\n",
    "    print(f\"{'upsample4':<25} {str(x.shape):<20} {'-':<10}\")\n",
    "    \n",
    "    x = model.bn5(model.conv5(x))\n",
    "    print(f\"{'conv5':<25} {str(x.shape):<20} {sum(p.numel() for p in model.conv5.parameters()):<10}\")\n",
    "      \n",
    "def generate_ground_truth_images(pi_data, grid_size=256):\n",
    "    ground_truth_images = []\n",
    "    for _, pi_info in pi_data.iterrows():\n",
    "        x_idx = int((pi_info['PI_Vx'] + grid_size / 2) * (grid_size / grid_size))\n",
    "        y_idx = int((pi_info['PI_Vy'] + grid_size / 2) * (grid_size / grid_size))\n",
    "        \n",
    "        ground_truth_image = np.zeros((grid_size, grid_size))\n",
    "        ground_truth_image[y_idx, x_idx] = 1\n",
    "        \n",
    "        ground_truth_images.append(ground_truth_image)\n",
    "        \n",
    "    return np.array(ground_truth_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c80c4b-ea02-4d68-b93d-3a22526193f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComptonNetV2()\n",
    "custom_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c865e1-562b-48ab-9058-75d7acf251eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_data = data_list['PI_data']\n",
    "incident_gamma_energy = 1.5 \n",
    "grid_size = 256\n",
    "\n",
    "processed_data_both = []\n",
    "processed_data_scatter = []\n",
    "processed_data_absorber = []\n",
    "ground_truth_images = []\n",
    "\n",
    "for event_id in data_list['SG_data']['SG_ID'].unique():\n",
    "    sg_mask = data_list['SG_data']['SG_ID'] == event_id\n",
    "    ag_mask = data_list['AG_data']['AG_ID'] == event_id\n",
    "    ce_mask = data_list['CE_data']['CE_ID'] == event_id\n",
    "    \n",
    "    event_both = [0, 0, 0, 0, 0, 0]\n",
    "    event_scatter = [0, 0, 0]\n",
    "    event_absorber = [0, 0, 0]\n",
    "    \n",
    "    if sg_mask.any():\n",
    "        sg_data = data_list['SG_data'][sg_mask].iloc[0]\n",
    "        event_scatter = [sg_data['SG_Vz'], sg_data['SG_Vy'], sg_data['SG_energy']]\n",
    "    \n",
    "    if ag_mask.any():\n",
    "        ag_data = data_list['AG_data'][ag_mask].iloc[0]\n",
    "        if 'SG_Back' in data_list['AG_vol']['VolName'][ag_mask].values[0]:\n",
    "            event_absorber = [ag_data['AG_Vz'], ag_data['AG_Vy'], sg_data['SG_energy']]\n",
    "\n",
    "    if sg_mask.any() and ag_mask.any() and ce_mask.any():\n",
    "        ag_layer_name = data_list['AG_vol']['VolName'][ag_mask].values[0]\n",
    "        if 'SG_Back' in ag_layer_name:\n",
    "            ce_data = data_list['CE_data'][ce_mask].iloc[0]\n",
    "            event_both = [sg_data['SG_Vz'], sg_data['SG_Vy'], ce_data['CE_energy'], ag_data['AG_Vz'], ag_data['AG_Vy'], sg_data['SG_energy']]\n",
    "    \n",
    "    processed_data_both.append(event_both)\n",
    "    processed_data_scatter.append(event_scatter)\n",
    "    processed_data_absorber.append(event_absorber)\n",
    "\n",
    "    \n",
    "    pi_mask = pi_data['PI_ID'] == event_id\n",
    "    pi_info = pi_data[pi_mask].iloc[0]\n",
    "    x_idx = int((pi_info['PI_Vx'] + grid_size / 2) * (grid_size / grid_size))\n",
    "    y_idx = int((pi_info['PI_Vy'] + grid_size / 2) * (grid_size / grid_size))\n",
    "\n",
    "    ground_truth_image = np.zeros((grid_size, grid_size))\n",
    "    ground_truth_image[y_idx, x_idx] = 1\n",
    "\n",
    "    ground_truth_images.append(ground_truth_image)\n",
    "\n",
    "columns_both = ['z_sca', 'y_sca', 'e_sca', 'z_abs', 'y_abs', 'e_abs']\n",
    "columns_scatter = ['z_sca', 'y_sca', 'e_sca']\n",
    "columns_absorber = ['z_abs', 'y_abs', 'e_abs']\n",
    "\n",
    "processed_df_both = pd.DataFrame(processed_data_both, columns=columns_both)\n",
    "processed_df_scatter = pd.DataFrame(processed_data_scatter, columns=columns_scatter)\n",
    "processed_df_absorber = pd.DataFrame(processed_data_absorber, columns=columns_absorber)\n",
    "\n",
    "input_data_both = torch.tensor(processed_df_both.values, dtype=torch.float32)\n",
    "input_data_scatter = torch.tensor(processed_df_scatter.values, dtype=torch.float32)\n",
    "input_data_absorber = torch.tensor(processed_df_absorber.values, dtype=torch.float32)\n",
    "\n",
    "ground_truth_images_tensor = torch.tensor(np.array(ground_truth_images), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "assert input_data_both.size(0) == input_data_scatter.size(0) == input_data_absorber.size(0) == ground_truth_images_tensor.size(0)\n",
    "\n",
    "dataset = TensorDataset(input_data_both, input_data_scatter, input_data_absorber, ground_truth_images_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b14e18-ef39-4a37-b64b-17eb3fd20010",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_both = torch.tensor(processed_df_both.values, dtype=torch.float32)\n",
    "input_data_scatter = torch.tensor(processed_df_scatter.values, dtype=torch.float32)\n",
    "input_data_absorber = torch.tensor(processed_df_absorber.values, dtype=torch.float32)\n",
    "\n",
    "ground_truth_images_tensor = torch.tensor(np.array(ground_truth_images), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "dataset = TensorDataset(input_data_both, input_data_scatter, input_data_absorber, ground_truth_images_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = ComptonNetV2()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9e8eb-3d05-4c14-b8c8-68e9ce37f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_visualize = 5\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs_both, inputs_scatter, inputs_absorber, labels) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs_both, inputs_scatter, inputs_absorber)\n",
    "        \n",
    "        if outputs.shape != labels.shape:\n",
    "            print(f\"Warning: Mismatch in output and label shapes: {outputs.shape} vs {labels.shape}\")\n",
    "            outputs = outputs.expand_as(labels)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(data_loader)}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs_both[:num_samples_to_visualize], inputs_scatter[:num_samples_to_visualize], inputs_absorber[:num_samples_to_visualize])\n",
    "            \n",
    "            # Make sure the output has the correct shape\n",
    "            if test_output.shape[1] != 1:\n",
    "                test_output = test_output.view(-1, 1, 256, 256)\n",
    "            \n",
    "            test_output_np = test_output.squeeze(1).cpu().numpy()  # Shape should be (N, 256, 256)\n",
    "            num_samples_in_batch = test_output_np.shape[0]  # Number of actual samples in this batch\n",
    "\n",
    "            for j in range(num_samples_in_batch):  # Only loop over available samples\n",
    "                plt.imshow(test_output_np[j], cmap='hot', interpolation='nearest')\n",
    "                plt.title(f\"Predicted Heatmap at Epoch {epoch}, Sample {j}\")\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "        model.train()\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs_both, inputs_scatter, inputs_absorber, _ = next(iter(data_loader))\n",
    "    output_images = model(inputs_both, inputs_scatter, inputs_absorber)\n",
    "    \n",
    "    for i in range(output_images.size(0)):\n",
    "        output_image = output_images[i].squeeze().cpu().numpy()\n",
    "        \n",
    "        max_position = np.unravel_index(np.argmax(output_image, axis=None), output_image.shape)\n",
    "        print(f\"Predicted source position for sample {i}: {max_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0564847-b1db-47b4-a3ff-d2d9f8139134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
